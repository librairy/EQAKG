{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-republican",
   "metadata": {},
   "source": [
    "# Wikidata Simple-Question with Answers\n",
    "\n",
    "We consider only questions with a single answer (predicate = 'P')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joint-river",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q7358590</td>\n",
       "      <td>P20</td>\n",
       "      <td>Q1637790</td>\n",
       "      <td>Where did roger marquis die</td>\n",
       "      <td>Holyoke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q154335</td>\n",
       "      <td>P509</td>\n",
       "      <td>Q12152</td>\n",
       "      <td>what was the cause of death of yves klein</td>\n",
       "      <td>myocardial infarction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q2747238</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q5059480</td>\n",
       "      <td>What position does carlos gomez play?</td>\n",
       "      <td>center fielder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q62498</td>\n",
       "      <td>P21</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>how does engelbert zaschka identify</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q182485</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q1143358</td>\n",
       "      <td>what position does pee wee reese play in baseball</td>\n",
       "      <td>shortstop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject predicate    object  \\\n",
       "0  Q7358590       P20  Q1637790   \n",
       "1   Q154335      P509    Q12152   \n",
       "2  Q2747238      P413  Q5059480   \n",
       "3    Q62498       P21  Q6581097   \n",
       "4   Q182485      P413  Q1143358   \n",
       "\n",
       "                                            question                 Answer  \n",
       "0                        Where did roger marquis die                Holyoke  \n",
       "1          what was the cause of death of yves klein  myocardial infarction  \n",
       "2              What position does carlos gomez play?         center fielder  \n",
       "3               how does engelbert zaschka identify                    male  \n",
       "4  what position does pee wee reese play in baseball              shortstop  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wikidata-sqa2.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-battlefield",
   "metadata": {},
   "source": [
    "### Named Entity Recognition based on Language Models, PoS tagging and Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tropical-grounds",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-12 11:50:33,823 loading file /home/jupyter-cbadenes/.flair/models/pos-english/a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n",
      "2022-04-12 11:50:34,397 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade --user pip\n",
    "#!pip install --user flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improving-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kung fu star']\n"
     ]
    }
   ],
   "source": [
    "def get_pos_entities(text,category):\n",
    "    # make example sentence\n",
    "    sentence = Sentence(text)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # print sentence\n",
    "    #print(sentence)\n",
    "    # iterate over entities and print\n",
    "    entities = []\n",
    "    current_entity = \"\"\n",
    "    for t in sentence.tokens:\n",
    "        for label in t.annotation_layers.keys():\n",
    "            text = t.text\n",
    "            label = t.get_labels(label)[0].value   \n",
    "            if (label == category):\n",
    "                if (current_entity == \"\"):\n",
    "                    current_entity += text\n",
    "                else:\n",
    "                    current_entity += \" \" + text\n",
    "            elif len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = \"\"\n",
    "          \n",
    "    if (len(current_entity)>0):\n",
    "        entities.append(current_entity)\n",
    "    return entities\n",
    "\n",
    "r = get_pos_entities(\"who's a kung fu star from hong kong\",\"NN\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regional-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "critical-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carl-alfred schumacher']\n"
     ]
    }
   ],
   "source": [
    "def get_entities(text):\n",
    "    entities = []\n",
    "    entity = \"\"\n",
    "    index = -1\n",
    "    offset = -1\n",
    "    for token in nlp(text):\n",
    "        if (index == -1):\n",
    "            index = token['index']\n",
    "            offset = token['start']\n",
    "        word = token['word']\n",
    "        if (word[0] == '#'):\n",
    "            word = token['word'].replace(\"#\",\"\")\n",
    "            \n",
    "        if (token['start']== offset):\n",
    "            entity += word\n",
    "        elif (token['index']-index < 2):\n",
    "            entity += \" \" + word\n",
    "        else:\n",
    "            entities.append(entity)\n",
    "            entity = word\n",
    "        index = token['index']\n",
    "        offset = token['end']\n",
    "        \n",
    "    if (len(entity) > 0):    \n",
    "        entities.append(entity)\n",
    "    if (len(entities) == 0):\n",
    "        cardinal_entities =  get_pos_entities(text,\"CD\")\n",
    "        if (len(cardinal_entities)>0):\n",
    "            return cardinal_entities\n",
    "        noun_entities =  get_pos_entities(text,\"NN\")\n",
    "        if (len(noun_entities)>0):\n",
    "            return noun_entities\n",
    "        \n",
    "    return entities\n",
    "\n",
    "r = get_entities(\"which city did carl-alfred schumacher die\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-galaxy",
   "metadata": {},
   "source": [
    "## Identification of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Where did roger marquis die\n",
      "\t entities: ['roger marquis']\n",
      "1 : what was the cause of death of yves klein\n",
      "\t entities: ['yves klein']\n",
      "2 : What position does carlos gomez play?\n",
      "\t entities: ['carlos gomez']\n",
      "3 : how does engelbert zaschka identify \n",
      "\t entities: ['engelbert zaschka']\n",
      "4 : what position does pee wee reese play in baseball\n",
      "\t entities: ['pee wee reese']\n",
      "5 : Which Swiss conductor's cause of death is myocardial infarction?\n",
      "\t entities: ['swiss']\n",
      "6 : where was padraic mcguinness's place of death\n",
      "\t entities: ['padraic mcguinness']\n",
      "7 : what is the place of birth of sam edwards?\n",
      "\t entities: ['sam edwards']\n",
      "8 : Which home is an example of italianate architecture?\n",
      "\t entities: ['italianate']\n",
      "9 : who published neo contra\n",
      "\t entities: ['neo contra']\n",
      "10 : what is angie estes's profession \n",
      "\t entities: ['angie estes']\n",
      "11 : what position does josé francisco torres play?\n",
      "\t entities: ['jose francisco torres']\n",
      "12 : what male actor was born in  warsaw\n",
      "\t entities: ['warsaw']\n",
      "13 : who was also born in jakarta\n",
      "\t entities: ['jakarta']\n",
      "14 : Who was born in prague\n",
      "\t entities: ['prague']\n",
      "15 : where was guy pnini born\n",
      "\t entities: ['guy pnini']\n",
      "16 : what is an album recorded by scott grimes\n",
      "\t entities: ['scott grimes']\n",
      "17 : what was the country of origin of the tv show sidewalks entertainment\n",
      "\t entities: ['sidewalks entertainment']\n",
      "18 : who was the architect of the structure tour perret\n",
      "\t entities: ['tour perret']\n",
      "19 : What is an album by guy clark?\n",
      "\t entities: ['guy clark']\n",
      "20 : what's the name of an Australian rock and roll\n",
      "\t entities: ['australian', 'roll']\n",
      "More than one entity found!\n",
      "21 : what is the category of the celestial object 1241 dysona\n",
      "\t entities: ['dysona']\n",
      "22 : What is a type of gameplay available to gamers playing custom robo v2\n",
      "\t entities: ['robo v']\n",
      "23 : What's an example of a romance film\n",
      "\t entities: ['example', 'romance film']\n",
      "More than one entity found!\n",
      "24 : where was shigeyasu suzuki's place of birth\n",
      "\t entities: ['shigeyasu suzuki']\n",
      "25 : which nationality is yonatan rozen\n",
      "\t entities: ['yonatan rozen']\n",
      "26 : which indian city was fazil born in\n",
      "\t entities: ['indian', 'fazil']\n",
      "More than one entity found!\n",
      "27 : where was paolo de la haza born?\n",
      "\t entities: ['paolo de la haza']\n",
      "28 : what gender is daniela cristofori \n",
      "\t entities: ['daniela cristofori']\n",
      "29 : in which country was overnight delivery filmed in\n",
      "\t entities: ['country', 'delivery']\n",
      "More than one entity found!\n",
      "30 : what is the second level division of the division crixás do tocantins\n",
      "\t entities: ['crixas do tocantins']\n",
      "31 : Which film was directed by ian iqbal rashid\n",
      "\t entities: ['ian iqbal rashid']\n",
      "32 : in what county is monango found\n",
      "\t entities: ['monango']\n",
      "33 : who recorded rca records\n",
      "\t entities: ['who', 'rca records']\n",
      "More than one entity found!\n",
      "34 : who is the artist of breathing your love?\n",
      "\t entities: ['artist', 'love']\n",
      "More than one entity found!\n",
      "35 : What is a gampelay mode found in the game allegiance\n",
      "\t entities: ['gampelay', 'allegiance']\n",
      "More than one entity found!\n",
      "36 : What type of film is raghuvinte swantham raziya\n",
      "\t entities: ['raghuvinte swantham raziya']\n",
      "37 : Which films has leonid gaidai written\n",
      "\t entities: ['which films', 'leonid gaidai']\n",
      "More than one entity found!\n",
      "38 : What is an organization that j. a. folger founded?\n",
      "\t entities: ['j. a. folger']\n",
      "39 : What is shinji mori's gender?\n",
      "\t entities: ['shinji mori']\n",
      "40 : what style of msuic did john pizzarelli play \n",
      "\t entities: ['msui', 'john pizzarelli']\n",
      "More than one entity found!\n",
      "41 : what does  2674 pandarus orbit\n",
      "\t entities: ['pandarus']\n",
      "42 : what is trond egil soltvedt's birth place \n",
      "\t entities: ['trond egil soltvedt']\n",
      "43 : what series is super mario bros. 2 a part of\n",
      "\t entities: ['super mario bros. 2']\n",
      "44 : what disease did anthony perkins die of\n",
      "\t entities: ['anthony perkins']\n",
      "45 : what is the mouth of the schuylkill river\n",
      "\t entities: ['schuylkill']\n",
      "46 : Which genre of book is bin laden: the man who declared war on america?\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "for index,row in df.iterrows():\n",
    "    question = row['question']\n",
    "    print(index,\":\",question)\n",
    "    q_entities = get_entities(question)\n",
    "    print(\"\\t entities:\",q_entities)\n",
    "    if (len(q_entities)<1):\n",
    "        print(\"No entities found!\")\n",
    "        entities.append(\"\")\n",
    "    elif (len(q_entities)>1):\n",
    "        print(\"More than one entity found!\")\n",
    "        entities.append(q_entities)\n",
    "    else:        \n",
    "        entities.append(q_entities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity']=entities\n",
    "df.to_csv('wikidata-sqa-e.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-shaft",
   "metadata": {},
   "source": [
    "### Wikidata Entity Linking based on MediaWiki API\n",
    "\n",
    "The MediaWiki Action API is a web service that allows access to some wiki-features like authentication, page operations, and search. It can provide meta information about the wiki and the logged-in user.\n",
    "\n",
    "action=wbsearchentities\n",
    "\n",
    "Searches for entities using labels and aliases.\n",
    "\n",
    "Returns a label and description for the entity in the user language if possible. Returns details of the matched term. The matched term text is also present in the aliases key if different from the display label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_wikidata_candidates(label):\n",
    "    candidates = []\n",
    "    if (label==\"\"):\n",
    "        return candidates\n",
    "    # type: One of the following values: form, form, item, lexeme, property, sense, sense\n",
    "    query_path = \"https://www.wikidata.org/w/api.php?action=wbsearchentities&search=QUERY_TEXT&language=en&limit=10&type=item&format=json\"\n",
    "    r = requests.get(query_path.replace(\"QUERY_TEXT\",label))\n",
    "    \n",
    "    for answer in r.json()['search']:\n",
    "        candidate = {\n",
    "            'label': answer['display']['label']['value'],\n",
    "            'id':answer['id']\n",
    "#            'description' : answer['display']['description']\n",
    "        }\n",
    "        candidates.append(candidate)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-profit",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "wikidata_items = []\n",
    "for index,row in df.iterrows():\n",
    "    question = row['question']\n",
    "    print(index,\":\",question)\n",
    "    q_entities = get_entities(question)\n",
    "    print(\"\\t entities:\",q_entities)\n",
    "    if (len(q_entities)<1):\n",
    "        print(\"No entities found!\")\n",
    "        entities.append(\"\")\n",
    "        wikidata_item.append(\"\")\n",
    "    elif (len(q_entities)>1):\n",
    "        print(\"More than one entity found!\")\n",
    "        entities.append(q_entities)\n",
    "    else:        \n",
    "        entities.append(q_entities[0])\n",
    "    q_wiki_entities = []    \n",
    "    for entity in q_entities:\n",
    "        for item in get_wikidata_candidates(entity):\n",
    "            q_wiki_entities.append(item['id'])\n",
    "    print(\"\\t wiki:\",q_wiki_entities)\n",
    "    wikidata_items.append(q_wiki_entities)\n",
    "    print(\"\\t reference:\",row['subject'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wikidata']=wikidata_items\n",
    "df.to_csv('wikidata-sqa-ew.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = df['subject']\n",
    "y_pred = df['wikidata']\n",
    "confusion_matrix(y_true, y_pred, labels=df['entity'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-redhead",
   "metadata": {},
   "source": [
    "### DBpedia Entity Linking based on DBpedia Lookup service\n",
    "\n",
    "The DBpedia Lookup is an entity retrieval service for Linked Data. It provides a straightforward solution for the frequent use case of resolving keywords and natural language to related resource identifiers in the DBpedia knowledge graph. Related means that either the label or abstract of a resource matches, or an anchor text that was frequently used in Wikipedia to refer to a specific resource matches (e.g. the resource http://dbpedia.org/resource/United_States can be looked up by the string “USA”). \n",
    "\n",
    "So whether you need an auto-complete service for your RDF application, Linked Data enhancements for your CSV tables or simply a way to retrieve specific DBpedia identifiers – the DBpedia Lookup is for you!\n",
    "\n",
    "As a part of the DBpedia Technology Stack the DBpedia Lookup can be deployed conveniently via Docker and works well with DBpedia Databus Collections. The DBpedia Lookup uses an Apache Lucene Index for resource indexing and retrieval and provides a web interface for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pressing-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '<B>Berlin</B>', 'id': 'http://dbpedia.org/resource/Berlin'}, {'label': '<B>Bergen</B>', 'id': 'http://dbpedia.org/resource/Bergen'}, {'label': 'Tennis Borussia <B>Berlin</B>', 'id': 'http://dbpedia.org/resource/Tennis_Borussia_Berlin'}, {'label': '1. FC Union <B>Berlin</B>', 'id': 'http://dbpedia.org/resource/1._FC_Union_Berlin'}, {'label': 'West <B>Berlin</B>', 'id': 'http://dbpedia.org/resource/West_Berlin'}, {'label': 'East <B>Berlin</B>', 'id': 'http://dbpedia.org/resource/East_Berlin'}, {'label': '<B>Berlín</B>, Usulután', 'id': 'http://dbpedia.org/resource/Berlín,_Usulután'}, {'label': 'Humboldt University of <B>Berlin</B>', 'id': 'http://dbpedia.org/resource/Humboldt_University_of_Berlin'}, {'label': 'Hertha BSC', 'id': 'http://dbpedia.org/resource/Hertha_BSC'}, {'label': 'Carlos <B>Berlín</B> Montero', 'id': 'http://dbpedia.org/resource/Carlos_Berlín_Montero'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def get_dbpedia_candidates(label):\n",
    "    candidates = []\n",
    "    if (label==\"\"):\n",
    "        return candidates\n",
    "    # type: One of the following values: form, form, item, lexeme, property, sense, sense\n",
    "    query_path = \"http://lookup.dbpedia.org/api/search?query=QUERY_TEXT&maxResults=10&type=class&format=json\"\n",
    "    r = requests.get(query_path.replace(\"QUERY_TEXT\",label))\n",
    "    for answer in r.json()['docs']:\n",
    "        candidate = {\n",
    "            'label': answer['label'][0],\n",
    "            'id':answer['resource'][0]\n",
    "#            'description' : answer['display']['description']\n",
    "        }\n",
    "        candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "r = get_dbpedia_candidates(\"Berlín\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-corps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
