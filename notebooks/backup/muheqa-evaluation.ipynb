{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "linear-riding",
   "metadata": {},
   "source": [
    "# Wikidata Simple-Question with Answers\n",
    "\n",
    "We consider only questions with a single answer (predicate = 'P')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wikidata-sqa2.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-document",
   "metadata": {},
   "source": [
    "### Named Entity Recognition based on Language Models, PoS tagging and Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-rates",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade --user pip\n",
    "#!pip install --user flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_entities(text,category):\n",
    "    # make example sentence\n",
    "    sentence = Sentence(text)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # print sentence\n",
    "    #print(sentence)\n",
    "    # iterate over entities and print\n",
    "    entities = []\n",
    "    current_entity = \"\"\n",
    "    for t in sentence.tokens:\n",
    "        for label in t.annotation_layers.keys():\n",
    "            text = t.text\n",
    "            label = t.get_labels(label)[0].value   \n",
    "            if (label == category):\n",
    "                if (current_entity == \"\"):\n",
    "                    current_entity += text\n",
    "                else:\n",
    "                    current_entity += \" \" + text\n",
    "            elif len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = \"\"\n",
    "          \n",
    "    if (len(current_entity)>0):\n",
    "        entities.append(current_entity)\n",
    "    return entities\n",
    "\n",
    "r = get_pos_entities(\"who's a kung fu star from hong kong\",\"NN\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    entities = []\n",
    "    entity = \"\"\n",
    "    index = -1\n",
    "    offset = -1\n",
    "    for token in nlp(text):\n",
    "        if (index == -1):\n",
    "            index = token['index']\n",
    "            offset = token['start']\n",
    "        word = token['word']\n",
    "        if (word[0] == '#'):\n",
    "            word = token['word'].replace(\"#\",\"\")\n",
    "            \n",
    "        if (token['start']== offset):\n",
    "            entity += word\n",
    "        elif (token['index']-index < 2):\n",
    "            entity += \" \" + word\n",
    "        else:\n",
    "            entities.append(entity)\n",
    "            entity = word\n",
    "        index = token['index']\n",
    "        offset = token['end']\n",
    "        \n",
    "    if (len(entity) > 0):    \n",
    "        entities.append(entity)\n",
    "    if (len(entities) == 0):\n",
    "        cardinal_entities =  get_pos_entities(text,\"CD\")\n",
    "        if (len(cardinal_entities)>0):\n",
    "            return cardinal_entities\n",
    "        noun_entities =  get_pos_entities(text,\"NN\")\n",
    "        if (len(noun_entities)>0):\n",
    "            return noun_entities\n",
    "        \n",
    "    return entities\n",
    "\n",
    "r = get_entities(\"which city did carl-alfred schumacher die\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-express",
   "metadata": {},
   "source": [
    "### Wikidata Entity Linking based on MediaWiki API\n",
    "\n",
    "The MediaWiki Action API is a web service that allows access to some wiki-features like authentication, page operations, and search. It can provide meta information about the wiki and the logged-in user.\n",
    "\n",
    "action=wbsearchentities\n",
    "\n",
    "Searches for entities using labels and aliases.\n",
    "\n",
    "Returns a label and description for the entity in the user language if possible. Returns details of the matched term. The matched term text is also present in the aliases key if different from the display label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_wikidata_candidates(label):\n",
    "    candidates = []\n",
    "    if (label==\"\"):\n",
    "        return candidates\n",
    "    # type: One of the following values: form, form, item, lexeme, property, sense, sense\n",
    "    query_path = \"https://www.wikidata.org/w/api.php?action=wbsearchentities&search=QUERY_TEXT&language=en&limit=10&type=item&format=json\"\n",
    "    r = requests.get(query_path.replace(\"QUERY_TEXT\",label))\n",
    "    \n",
    "    for answer in r.json()['search']:\n",
    "        candidate = {\n",
    "            'label': answer['display']['label']['value'],\n",
    "            'id':answer['id']\n",
    "#            'description' : answer['display']['description']\n",
    "        }\n",
    "        candidates.append(candidate)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-charlotte",
   "metadata": {},
   "source": [
    "## Identification of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for index,row in df.iterrows():\n",
    "    question = row['question']\n",
    "    print(index,\":\",question)\n",
    "    q_entities = get_entities(question)\n",
    "    print(\"\\t entities:\",q_entities)\n",
    "    if (len(q_entities)<1):\n",
    "        print(\"No entities found!\")\n",
    "        entities.append(\"\")\n",
    "    elif (len(q_entities)>1):\n",
    "        print(\"More than one entity found!\")\n",
    "        entities.append(q_entities)\n",
    "    else:        \n",
    "        entities.append(q_entities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity']=entities\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-victory",
   "metadata": {},
   "source": [
    "## Wikidata linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-liquid",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "wikidata_items = []\n",
    "for index,row in df.iterrows():\n",
    "    question = row['question']\n",
    "    print(index,\":\",question)\n",
    "    q_entities = get_entities(question)\n",
    "    print(\"\\t entities:\",q_entities)\n",
    "    if (len(q_entities)<1):\n",
    "        print(\"No entities found!\")\n",
    "        entities.append(\"\")\n",
    "        wikidata_item.append(\"\")\n",
    "    elif (len(q_entities)>1):\n",
    "        print(\"More than one entity found!\")\n",
    "        entities.append(q_entities)\n",
    "    else:        \n",
    "        entities.append(q_entities[0])\n",
    "    q_wiki_entities = []    \n",
    "    for entity in q_entities:\n",
    "        for item in get_wikidata_candidates(entity):\n",
    "            q_wiki_entities.append(item['id'])\n",
    "    print(\"\\t wiki:\",q_wiki_entities)\n",
    "    wikidata_items.append(q_wiki_entities)\n",
    "    print(\"\\t reference:\",row['subject'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity']=entities\n",
    "df['wikidata']=wikidata_items\n",
    "df.to_csv('wikidata-sqa-ew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = df['subject']\n",
    "y_pred = df['wikidata']\n",
    "confusion_matrix(y_true, y_pred, labels=df['entity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = get_entities(\"what type of celestial object is (101180) 1998 sh9\")\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
